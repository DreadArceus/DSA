Since execution time changes depending on the machine and network status,
  we need a way to judge the time taken by a given program/algorithm to run.
Hence, we define "Time Complexity" as the number of statements that have to execute for the program to run.
Eg. A for loop from 0 to n-1 makes the statements inside execute n times,
  so the time complexity is said to be linear
Due to this, time complexity scales time with respect to the inputs taken during the program execution

Big O: (Asymptotic Notation)
this notation is used to denote the time complexity,
O(1) : constant TC
O(n) : linear TC
O(n^2) : quadratic TC
O(n^3) : cubic TC
O(logn) : logarithmic TC
O(n!) : factorial TC
O(a^n) : exponential TC
O(n^y) : polynomial TC

When judging the time complexity for a program the worst complexity from the constituting code is taken.
  ie a program is only as fast as its slowest part

If X has better time complexity than Y, then X is always better than Y for large inputs.

(n : some input integer)
